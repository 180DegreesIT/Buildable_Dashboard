---
phase: 02-excel-data-migration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/src/services/ExcelParserService.ts
  - server/src/services/SheetParsers/WeeklyReportParser.ts
  - server/src/services/SheetParsers/RevenueReportParser.ts
  - server/src/services/SheetParsers/FinanceThisWeekParser.ts
  - server/src/services/SheetParsers/ProductivityParser.ts
  - server/src/services/SheetParsers/PhoneParser.ts
  - server/src/services/SheetParsers/MarketingParser.ts
  - server/src/services/ExcelMigrationService.ts
  - server/src/routes/migration.ts
  - server/src/index.ts
autonomous: true

must_haves:
  truths:
    - "Uploading the Excel workbook via POST /api/v1/migration/upload returns parsed data preview with row counts per table"
    - "Running dry-run via POST /api/v1/migration/dry-run returns parsed records grouped by target table with warnings for formula errors and uncached references"
    - "Running import via POST /api/v1/migration/import upserts all parsed records into correct database tables with data_source 'backfilled'"
    - "Running import a second time produces identical row counts (idempotent -- no duplicate rows)"
    - "Formula errors (#DIV/0!) and uncached cross-sheet formulas are set to 0 with logged warnings, not aborting the migration"
    - "GET /api/v1/migration/progress/:jobId streams SSE events with real-time phase, sheet, and record count updates"
    - "All imported dates are Saturdays (Sundays from workbook snapped to previous Saturday via WeekService.toSaturday)"
  artifacts:
    - path: "server/src/services/ExcelParserService.ts"
      provides: "Cell value extraction utility handling 7+ ExcelJS cell types (formula errors, uncached formulas, hyperlinks, richtext, dates, numbers, strings)"
      exports: ["extractCell", "CellExtraction", "parseTransposedSheet", "TransposedConfig"]
    - path: "server/src/services/SheetParsers/WeeklyReportParser.ts"
      provides: "Parser for Sheet 1 Weekly Report -> financial_weekly, projects_weekly, sales_weekly, leads_weekly, google_reviews_weekly, team_performance_weekly"
      exports: ["WeeklyReportParser"]
    - path: "server/src/services/SheetParsers/RevenueReportParser.ts"
      provides: "Parser for Sheet 7 Weekly Revenue Report -> revenue_weekly (standard table layout, not transposed)"
      exports: ["RevenueReportParser"]
    - path: "server/src/services/SheetParsers/FinanceThisWeekParser.ts"
      provides: "Parser for Sheet 6 Finance This Week -> cash_position_weekly (single week snapshot)"
      exports: ["FinanceThisWeekParser"]
    - path: "server/src/services/SheetParsers/ProductivityParser.ts"
      provides: "Parser for Sheet 12 Productivity -> staff_productivity_weekly"
      exports: ["ProductivityParser"]
    - path: "server/src/services/SheetParsers/PhoneParser.ts"
      provides: "Parser for Sheet 16 Phone (2) -> phone_weekly"
      exports: ["PhoneParser"]
    - path: "server/src/services/SheetParsers/MarketingParser.ts"
      provides: "Parser for Sheets 9+10 Marketing Weekly APP/BA -> marketing_performance_weekly"
      exports: ["MarketingParser"]
    - path: "server/src/services/ExcelMigrationService.ts"
      provides: "Migration orchestrator: reads workbook, invokes all parsers, manages progress events, performs upserts"
      exports: ["ExcelMigrationService", "MigrationResult", "MigrationProgress"]
    - path: "server/src/routes/migration.ts"
      provides: "Express routes for upload, dry-run, import, SSE progress"
      exports: ["default (Router)"]
    - path: "server/src/index.ts"
      provides: "Migration route registered at /api/v1/migration"
  key_links:
    - from: "server/src/services/ExcelParserService.ts"
      to: "exceljs"
      via: "ExcelJS Cell.value type handling"
      pattern: "extractCell.*CellValue"
    - from: "server/src/services/SheetParsers/*.ts"
      to: "server/src/services/ExcelParserService.ts"
      via: "import { extractCell, parseTransposedSheet }"
      pattern: "import.*ExcelParserService"
    - from: "server/src/services/ExcelMigrationService.ts"
      to: "server/src/services/SheetParsers/*.ts"
      via: "Orchestrator calls each parser"
      pattern: "WeeklyReportParser|RevenueReportParser|FinanceThisWeekParser"
    - from: "server/src/services/ExcelMigrationService.ts"
      to: "server/src/services/WeekService.ts"
      via: "Date snapping to Saturday"
      pattern: "WeekService\\.toSaturday"
    - from: "server/src/services/ExcelMigrationService.ts"
      to: "prisma upsert"
      via: "Idempotent database writes with unique constraints"
      pattern: "prisma\\..*\\.upsert"
    - from: "server/src/routes/migration.ts"
      to: "server/src/services/ExcelMigrationService.ts"
      via: "Route handlers call migration service"
      pattern: "ExcelMigrationService"
    - from: "server/src/index.ts"
      to: "server/src/routes/migration.ts"
      via: "app.use('/api/v1/migration', migrationRoutes)"
      pattern: "migration"
---

<objective>
Build the complete Excel migration backend: cell extraction utility, 6 sheet-specific parsers, migration orchestrator service, and Express API routes with SSE progress streaming.

Purpose: Enable the server to parse the "Weekly Report - 30.xlsx" workbook, extract 30 weeks of historical data from 8 importable sheets, and upsert it into 10+ database tables idempotently. This is the backend foundation that the migration UI (Plan 02) will consume.

Output: Working API endpoints that accept .xlsx upload, return dry-run preview with warnings, stream real-time progress, and perform idempotent database imports.
</objective>

<execution_context>
@C:/Users/Dev180D/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dev180D/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-excel-data-migration/02-RESEARCH.md
@server/prisma/schema.prisma
@server/src/services/WeekService.ts
@server/src/services/ImportService.ts
@server/src/routes/uploads.ts
@server/src/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ExcelParserService and all 6 sheet parsers</name>
  <files>
    server/src/services/ExcelParserService.ts
    server/src/services/SheetParsers/WeeklyReportParser.ts
    server/src/services/SheetParsers/RevenueReportParser.ts
    server/src/services/SheetParsers/FinanceThisWeekParser.ts
    server/src/services/SheetParsers/ProductivityParser.ts
    server/src/services/SheetParsers/PhoneParser.ts
    server/src/services/SheetParsers/MarketingParser.ts
  </files>
  <action>
    First, install ExcelJS:
    ```bash
    cd server && npm install exceljs
    ```

    **ExcelParserService.ts** — Low-level cell extraction and shared transposed-sheet parsing utility.

    Create `extractCell(cellValue: CellValue, ref: string): CellExtraction` function that handles ALL ExcelJS cell value types:
    - `null`/`undefined` → `{ value: null }`
    - `number` → `{ value: number }`
    - `string` → `{ value: string.trim() }`
    - `boolean` → `{ value: 1 or 0 }`
    - `Date` → `{ value: Date }`
    - Formula error `{ error: '#DIV/0!' }` → `{ value: 0, warning: "${ref}: Formula error #DIV/0!, set to 0" }`
    - Formula with cached result `{ formula: '...', result: X }` → extract result recursively (could be another error)
    - Uncached formula `{ formula: '...', NO result key }` → `{ value: 0, warning: "${ref}: Uncached formula '${formula}', set to 0" }`
    - Rich text `{ richText: [...] }` → concatenate .text from each item, trim
    - Hyperlink `{ text: '...', hyperlink: '...' }` → extract .text, trim
    - Fallback → `String(val).trim()`

    Create `CellExtraction` interface: `{ value: number | string | Date | null; warning?: string }`

    Create `parseTransposedSheet()` generic utility:
    ```typescript
    interface TransposedConfig {
      dateRow: number;
      startCol: number;
      labelCol: number;
      rowMappings: Array<{ row: number; dbField: string; type: 'currency' | 'integer' | 'decimal' | 'percentage' }>;
    }
    interface ParsedWeek { weekDate: Date; values: Record<string, number | null>; warnings: string[] }
    function parseTransposedSheet(ws: ExcelJS.Worksheet, config: TransposedConfig): ParsedWeek[]
    ```
    This iterates columns from `startCol` to the last non-empty date column. For each column: extract the date from `dateRow`, snap to Saturday via `WeekService.toSaturday()`, extract each mapped row value via `extractCell()`. Skip columns where ALL values are null (empty future weeks). Collect warnings.

    Also export `extractNumericValue(cellValue: CellValue, ref: string): { value: number | null; warning?: string }` — calls extractCell then coerces to number (parseFloat for strings, 0 for errors).

    **WeeklyReportParser.ts** — Parser for Sheet 1 "Weekly Report" (transposed).

    This is the largest parser — it extracts data for 6 tables from one sheet. Create a class `WeeklyReportParser` with a `parse(workbook: ExcelJS.Workbook)` method that returns:
    ```typescript
    interface WeeklyReportResult {
      financial: ParsedWeek[];
      projects: Array<{ weekDate: Date; projectType: string; values: Record<string, number | null>; warnings: string[] }>;
      sales: Array<{ weekDate: Date; salesType: string; values: Record<string, number | null>; warnings: string[] }>;
      leads: Array<{ weekDate: Date; source: string; values: Record<string, number | null>; warnings: string[] }>;
      googleReviews: ParsedWeek[];
      teamPerformance: Array<{ weekDate: Date; region: string; values: Record<string, number | null>; warnings: string[] }>;
    }
    ```

    Row mappings from research (Sheet 1, "Weekly Report"):

    **financial_weekly** (date row 3, label col 1, data starts col 3):
    - Row 4: totalTradingIncome (currency)
    - Row 5: totalCostOfSales (currency)
    - Row 6: grossProfit (currency)
    - Row 7: otherIncome (currency)
    - Row 8: operatingExpenses (currency)
    - Row 9: wagesAndSalaries (currency)
    - Row 10: netProfit (currency)

    **projects_weekly** — 3 project types, each with its own rows:
    - Residential: hyperfloCount (row 18, integer), xeroInvoicedAmount (row 19, currency), newBusinessPercentage (row 21, percentage)
    - Commercial: hyperfloCount (row 26, integer), xeroInvoicedAmount (row 27, currency)
    - Retrospective: hyperfloCount (row 30, integer), xeroInvoicedAmount (row 31, currency)

    **sales_weekly** — 3 sales types:
    - Residential: quotesIssuedCount (row 35, integer), quotesIssuedValue (row 36, currency), quotesWonCount (row 37, integer), quotesWonValue (row 38, currency)
    - Commercial: quotesIssuedCount (row 41, integer), quotesIssuedValue (row 42, currency), quotesWonCount (row 43, integer), quotesWonValue (row 44, currency)
    - Retrospective: quotesIssuedCount (row 47, integer), quotesIssuedValue (row 48, currency), quotesWonCount (row 49, integer), quotesWonValue (row 50, currency)

    **leads_weekly** — 6 lead sources (pairs of rows: count + costPerLead):
    - Google: leadCount (row 55), costPerLead (row 56)
    - SEO: leadCount (row 57), costPerLead (row 58)
    - Meta: leadCount (row 59), costPerLead (row 60)
    - Bing: leadCount (row 61), costPerLead (row 62)
    - TikTok: leadCount (row 63), costPerLead (row 64)
    - Other: leadCount (row 65), costPerLead (row 66)

    For each lead source, calculate `totalCost = leadCount * costPerLead` (if both are non-null).

    **google_reviews_weekly** — single row:
    - reviewCount (row 70, integer)

    **team_performance_weekly** — 9 regions, pairs of rows (target + actual):
    - Cairns: target (row 73), actualInvoiced (row 74)
    - Mackay: target (row 76), actualInvoiced (row 77)
    - NQ Commercial: target (row 79), actualInvoiced (row 80)
    - SEQ Residential: target (row 82), actualInvoiced (row 83)
    - SEQ Commercial: target (row 85), actualInvoiced (row 86)
    - Town Planning: target (row 88), actualInvoiced (row 89)
    - Townsville: target (row 91), actualInvoiced (row 92)
    - Wide Bay: target (row 94), actualInvoiced (row 95)
    - All In Access: target (row 97), actualInvoiced (row 98)

    Note: team_performance_weekly only stores `actualInvoiced` (not target — targets are in the Target table). Parse both for the dry-run preview but only import actualInvoiced.

    Skip rows 100-108 (Approvable Retro, Toowoomba, combined regions — not standard regions).

    **RevenueReportParser.ts** — Parser for Sheet 7 "Weekly Revenue Report" (standard table layout, NOT transposed).

    This sheet has rows as weeks and columns as revenue categories. Date is in column 1. Create class `RevenueReportParser` with `parse(workbook)` method.

    Column-to-RevenueCategory mapping:
    - Col 2: class_1a
    - Col 3: class_10a_sheds
    - Col 4: class_10b_pools
    - Col 5: inspections
    - Col 6: retrospective
    - Col 7: class_2_9_commercial
    - Col 8: planning_1_10
    - Col 28: access_labour_hire

    Skip total columns (9, 17, 29, 30) — they are calculated sums. Skip any column that maps to a category already mapped by another column. Dates are already Saturdays in this sheet (confirmed in research). Return array of `{ weekDate, category, amount, warnings }`.

    **FinanceThisWeekParser.ts** — Parser for Sheet 6 "Finance This Week" (single-week snapshot).

    Returns exactly 1 record for cash_position_weekly. Parse specific cells:
    - Row 8: ANZ Everyday account balance
    - Row 10: NAB Everyday account balance
    - Combine rows 8 + 10 as `everydayAccount`
    - Row 11: taxSavings (NAB Tax Account)
    - Row 12: capitalAccount (Profit Savings)
    - Row 17: creditCards (CC Debt/Credit ANZ)
    - Row 18: totalCashAvailable
    - Row 22, col A: totalReceivables
    - Row 22, col B: currentReceivables
    - Row 22, col C: over30Days
    - Row 22, col D: over60Days
    - Row 22, col E: over90Days
    - Row 25: totalPayables

    For the week date, use the most recent Saturday from the main Weekly Report sheet (passed in as parameter). This sheet has no date column.

    **ProductivityParser.ts** — Parser for Sheet 12 "Productivity" (transposed).

    Row structure: groups of 3 rows per staff member (# count, $ revenue, inspections). Column B has the clean staff name. Date row is row 3. Data starts at column D (col 4).

    Parse staff in sections:
    - Section header "Certifiers (sign off user)" → role: `certifier`
    - Section header "Cadets" → role: `cadet`

    For each staff member, create one `staff_productivity_weekly` record per week with:
    - staffName (from column B of the first row in the group)
    - role (from section header)
    - jobsCompleted (# row value, integer)
    - revenueGenerated ($ row value, currency)
    - inspectionsCompleted (inspections row value, integer)

    Skip the "Average" column (column C). Skip empty staff name rows. Many cells will be uncached formulas (468 in this sheet) — `extractCell` handles these.

    **PhoneParser.ts** — Parser for Sheet 16 "Phone (2)" (transposed, simpler than Sheet 17).

    Use Phone (2), not Phone (628 errors). Row structure: staff name rows with inbound/outbound/missed calls per week. Date row 3, data starts col 3.

    For each staff member, parse:
    - staffName (from label column)
    - inboundCalls (integer)
    - outboundCalls (integer)
    - missedCalls (integer)

    **MarketingParser.ts** — Parser for Sheets 9 "Marketing Weekly APP" + 10 "Marketing Weekly BA" (both transposed).

    Parse both sheets and **combine metrics per platform per week** (sum APP + BA values). Map platform rows to MarketingPlatform enum:
    - Google Ads rows → `google_ads`
    - Meta/Facebook rows → `meta_ads`
    - Bing rows → `bing_ads`
    - TikTok rows → `tiktok_ads`
    - SEO rows → `seo`

    Fields per platform: impressions, clicks, cost, conversions. Calculate CTR and CPC if data available: `ctr = clicks / impressions`, `cpc = cost / clicks`.

    **IMPORTANT for all parsers:**
    - Every parser must return warnings as an array of strings (from extractCell)
    - Every parser's parse() method takes the ExcelJS Workbook object
    - Use `workbook.getWorksheet('Sheet Name')` to access sheets by name. If sheet not found, return empty result with warning.
    - All parsers use extractCell/extractNumericValue from ExcelParserService
    - Snap ALL dates to Saturday via WeekService.toSaturday()
    - Skip columns with all-null data (future week placeholders)
  </action>
  <verify>
    ```bash
    cd server && npx tsc --noEmit 2>&1 | head -50
    ```
    TypeScript compilation should pass (or only show pre-existing Prisma-related errors, not new errors from these files). Verify exceljs is in package.json:
    ```bash
    cd server && cat package.json | grep exceljs
    ```
  </verify>
  <done>
    ExcelParserService exports extractCell and parseTransposedSheet. Six parser files exist in SheetParsers/ directory, each exporting a parser class with a parse(workbook) method. All parsers handle formula errors, uncached formulas, and date snapping. ExcelJS is installed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ExcelMigrationService orchestrator, migration API routes, and register in server</name>
  <files>
    server/src/services/ExcelMigrationService.ts
    server/src/routes/migration.ts
    server/src/index.ts
  </files>
  <action>
    **ExcelMigrationService.ts** — Main migration orchestrator.

    Create a class with these methods:

    1. `async parseWorkbook(filePath: string | Buffer): Promise<DryRunResult>` — Opens the workbook with ExcelJS, calls all 6 parsers, aggregates results into a dry-run preview.

    ```typescript
    interface DryRunResult {
      tables: Array<{
        tableName: string;
        recordCount: number;
        sampleRecords: Array<Record<string, any>>; // First 3 records for preview
        warnings: string[];
      }>;
      totalRecords: number;
      totalWarnings: number;
      allWarnings: string[];
    }
    ```

    The orchestrator should:
    - Load workbook: `const wb = new ExcelJS.Workbook(); await wb.xlsx.readFile(filePath)` or `await wb.xlsx.load(buffer)`
    - Call each parser, catch errors per parser (if one sheet fails, continue with others)
    - Aggregate all parsed records grouped by target table
    - Return dry-run preview with sample records and all warnings

    2. `async importData(filePath: string | Buffer, jobId: string): Promise<MigrationResult>` — Full import with progress.

    ```typescript
    interface MigrationResult {
      success: boolean;
      tables: Array<{
        tableName: string;
        inserted: number;
        updated: number;
        warnings: string[];
      }>;
      totalInserted: number;
      totalUpdated: number;
      totalWarnings: number;
      allWarnings: string[];
    }
    ```

    This method:
    - Parses the workbook (reuse parseWorkbook internally)
    - For each table group, performs Prisma upserts using the table's unique constraint
    - Emits progress events via an EventEmitter (see below)
    - Sets `dataSource: 'backfilled'` on all records

    **Progress system**: Use a module-level `EventEmitter` (Node.js native):
    ```typescript
    import { EventEmitter } from 'events';
    export const migrationEmitter = new EventEmitter();
    migrationEmitter.setMaxListeners(20);

    interface ProgressEvent {
      phase: 'parsing' | 'importing' | 'complete' | 'error';
      sheet?: string;
      table?: string;
      current: number;
      total: number;
      warnings: number;
      message: string;
    }
    ```

    Call `migrationEmitter.emit(jobId, progressEvent)` at these points:
    - When starting to parse each sheet
    - When starting to import to each table
    - After every 5 records imported (batched updates)
    - On completion
    - On error

    **Upsert pattern** for each table (example for financial_weekly):
    ```typescript
    await prisma.financialWeekly.upsert({
      where: { weekEnding: weekDate },
      update: { ...fields, dataSource: 'backfilled' },
      create: { weekEnding: weekDate, ...fields, dataSource: 'backfilled' },
    });
    ```

    Use the TABLE_UNIQUE_KEYS pattern from ImportService.ts as reference for which fields form unique constraints. But don't reuse ImportService directly — the Excel migration has different data shapes (pre-parsed records vs CSV rows).

    For tables with composite unique keys (projects_weekly: weekEnding + projectType, leads_weekly: weekEnding + source, etc.), use Prisma's compound unique where:
    ```typescript
    await prisma.projectsWeekly.upsert({
      where: { weekEnding_projectType: { weekEnding: weekDate, projectType: 'residential' } },
      update: { ...fields, dataSource: 'backfilled' },
      create: { weekEnding: weekDate, projectType: 'residential', ...fields, dataSource: 'backfilled' },
    });
    ```

    **IMPORTANT:** Check the actual Prisma-generated compound unique field names. They follow the pattern `fieldA_fieldB` based on the `@@unique` annotation. For each model:
    - FinancialWeekly: `weekEnding` (single field)
    - ProjectsWeekly: `weekEnding_projectType`
    - SalesWeekly: `weekEnding_salesType`
    - TeamPerformanceWeekly: `weekEnding_region`
    - LeadsWeekly: `weekEnding_source`
    - GoogleReviewsWeekly: `weekEnding` (single field)
    - RevenueWeekly: `weekEnding_category`
    - StaffProductivityWeekly: `weekEnding_staffName`
    - PhoneWeekly: `weekEnding_staffName`
    - MarketingPerformanceWeekly: `weekEnding_platform`
    - CashPositionWeekly: `weekEnding` (single field)

    **migration.ts** — Express router with 4 endpoints:

    1. `POST /upload` — Accept .xlsx file upload via multer (separate config from CSV upload — accept .xlsx MIME type `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet`, max 20MB). Store file in memory buffer. Return `{ jobId: uuid, fileName }`.

    2. `POST /dry-run` — Accept `{ filePath?: string }` body OR use the most recently uploaded file buffer. Call `ExcelMigrationService.parseWorkbook()`. Return the DryRunResult. If filePath provided and file exists on disk, read from disk (for server-side file path support).

    Generate jobId with `crypto.randomUUID()` (Node.js built-in).

    For the multer config for .xlsx files:
    ```typescript
    const xlsxUpload = multer({
      storage: multer.memoryStorage(),
      limits: { fileSize: 20 * 1024 * 1024 },
      fileFilter: (_req, file, cb) => {
        const ext = file.originalname.toLowerCase().split('.').pop();
        if (ext === 'xlsx' || ext === 'xls') {
          cb(null, true);
        } else {
          cb(new Error('Only .xlsx and .xls files are accepted'));
        }
      },
    });
    ```

    Combine upload + dry-run into one endpoint for simplicity: `POST /upload` accepts file, parses it, returns dry-run preview. Store the buffer in a module-level Map keyed by jobId for the subsequent import step.

    ```typescript
    // In-memory store for uploaded workbook buffers (cleared after import or timeout)
    const pendingUploads = new Map<string, { buffer: Buffer; fileName: string; uploadedAt: Date }>();

    // Auto-cleanup after 30 minutes
    setInterval(() => {
      const now = Date.now();
      for (const [id, upload] of pendingUploads) {
        if (now - upload.uploadedAt.getTime() > 30 * 60 * 1000) {
          pendingUploads.delete(id);
        }
      }
    }, 5 * 60 * 1000);
    ```

    3. `POST /import/:jobId` — Trigger the actual import using the stored buffer. Call `ExcelMigrationService.importData()` in the background (don't await — let SSE stream progress). Return `{ status: 'started', jobId }` immediately.

    4. `GET /progress/:jobId` — SSE endpoint streaming progress events.
    ```typescript
    router.get('/progress/:jobId', (req, res) => {
      res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no',
      });
      res.flushHeaders();

      const handler = (data: any) => {
        res.write(`data: ${JSON.stringify(data)}\n\n`);
        if (data.phase === 'complete' || data.phase === 'error') {
          setTimeout(() => res.end(), 500);
        }
      };

      migrationEmitter.on(req.params.jobId, handler);
      req.on('close', () => migrationEmitter.off(req.params.jobId, handler));
    });
    ```

    **server/src/index.ts** — Register the migration route:

    Add import: `import migrationRoutes from './routes/migration.js';`
    Add route: `app.use('/api/v1/migration', migrationRoutes);`

    Place it with the other API routes, after the auth middleware.

    **Error handling:** Wrap all route handlers in try/catch. Use the existing ApiError pattern from errorHandler.js for user-facing errors. Log internal errors to console.error.
  </action>
  <verify>
    ```bash
    cd server && npx tsc --noEmit 2>&1 | head -50
    ```
    TypeScript compilation passes (or only pre-existing errors). Then verify routes are registered:
    ```bash
    cd server && grep -n "migration" src/index.ts
    ```
    Should show the import and the app.use line.

    Start the server and test the health check + migration route existence:
    ```bash
    cd C:/Projects/buildable_dashboard && npm run dev &
    sleep 5
    curl -s http://localhost:6001/api/health
    curl -s -X POST http://localhost:6001/api/v1/migration/upload
    ```
    The upload endpoint should return an error about missing file (not a 404), confirming the route is registered.
  </verify>
  <done>
    ExcelMigrationService orchestrates all 6 parsers, performs Prisma upserts with data_source 'backfilled', and emits SSE progress events. The migration route is registered at /api/v1/migration with upload, import, and progress endpoints. Running a dry-run on the workbook returns parsed data grouped by table. Running import a second time doesn't create duplicates (upserts overwrite).
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **TypeScript compilation:**
   ```bash
   cd server && npx tsc --noEmit
   ```
   No new errors from migration files.

2. **ExcelJS installed:**
   ```bash
   cd server && node -e "require('exceljs'); console.log('OK')"
   ```

3. **Route registration:**
   ```bash
   cd server && grep "migration" src/index.ts
   ```

4. **API endpoint test (with server running):**
   ```bash
   curl -s -X POST http://localhost:6001/api/v1/migration/upload | head -1
   ```
   Should return JSON error about missing file, not 404.

5. **Full integration test with actual workbook (if available):**
   ```bash
   curl -s -X POST -F "file=@C:/Users/Dev180D/Downloads/Client_Files/Reporting/Weekly Report - 30.xlsx" http://localhost:6001/api/v1/migration/upload
   ```
   Should return dry-run preview with table counts and warnings.

6. **Idempotency test:**
   Run the import twice against the same workbook. Row counts should be identical. No duplicate rows in any table.
</verification>

<success_criteria>
- ExcelJS parses the workbook without crashing
- All 8 importable sheets are parsed by their respective parsers
- Formula errors (#DIV/0!) produce value 0 + warning, not exceptions
- Uncached cross-sheet formulas produce value 0 + warning
- Sunday dates are snapped to Saturday
- 30 weeks of data parsed (not 52 template columns)
- Dry-run returns record counts and sample data for each target table
- Import upserts data with data_source 'backfilled'
- Re-running import produces identical results (idempotent)
- SSE progress endpoint streams parsing and import events
- Migration route registered at /api/v1/migration
</success_criteria>

<output>
After completion, create `.planning/phases/02-excel-data-migration/02-01-SUMMARY.md`
</output>
